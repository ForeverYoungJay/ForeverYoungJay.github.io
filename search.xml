<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Fourier transform and Gabor transform</title>
    <url>/2021/04/23/Fourier-transform-and-Gabor-transform/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/emouse/p/3611256.html">https://www.cnblogs.com/emouse/p/3611256.html</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Gaussian Filter &amp; Gaussian Pyramid</title>
    <url>/2021/05/17/Gaussian-Filter-Gaussian-Pyramid/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/qq_38131594/article/details/80758734">https://blog.csdn.net/qq_38131594/article/details/80758734</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Java没有main方法如何运行</title>
    <url>/2021/04/22/Java%E6%B2%A1%E6%9C%89main%E6%96%B9%E6%B3%95%E5%A6%82%E4%BD%95%E8%BF%90%E8%A1%8C/</url>
    <content><![CDATA[<p><strong>在该方法中构造main函数</strong></p>
<p>Sources 一般用于标注类似 src 这种可编译目录。有时候我们不单单项目的 src 目录要可编译，还有其他一些特别的目录也许我们也要作为可编译的目录，就需要对该目录进行此标注。只有 Sources 这种可编译目录才可以新建 Java 类和包，这一点需要牢记。<br>Tests 一般用于标注可编译的单元测试目录。在规范的 maven 项目结构中，顶级目录是 src，maven 的 src 我们是不会设置为 Sources 的，而是在其子目录 main 目录下的 java 目录，我们会设置为 Sources。而单元测试的目录是 src - test - java，这里的 java 目录我们就会设置为 Tests，表示该目录是作为可编译的单元测试目录。一般这个和后面几个我们都是在 maven 项目下进行配置的，但是我这里还是会先说说。从这一点我们也可以看出 IntelliJ IDEA 对 maven 项目的支持是比彻底的。<br>Resources 一般用于标注资源文件目录。在 maven 项目下，资源目录是单独划分出来的，其目录为：src - main -resources，这里的 resources 目录我们就会设置为 Resources，表示该目录是作为资源目录。资源目录下的文件是会被编译到输出目录下的。<br>Test Resources 一般用于标注单元测试的资源文件目录。在 maven 项目下，单元测试的资源目录是单独划分出来的，其目录为：src - test -resources，这里的 resources 目录我们就会设置为 Test Resources，表示该目录是作为单元测试的资源目录。资源目录下的文件是会被编译到输出目录下的。<br>Excluded 一般用于标注排除目录。被排除的目录不会被 IntelliJ IDEA 创建索引，相当于被 IntelliJ IDEA 废弃，该目录下的代码文件是不具备代码检查和智能提示等常规代码功能。<br>通过上面的介绍，我们知道对于非 maven 项目我们只要会设置 src 即可。</p>
]]></content>
  </entry>
  <entry>
    <title>Laplacian滤波器并用于图像锐化</title>
    <url>/2021/04/23/Laplacian%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%B9%B6%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E9%94%90%E5%8C%96/</url>
    <content><![CDATA[<p>拉普拉斯算子在平面内的数值近似<img src="https://upload-images.jianshu.io/upload_images/17221499-3e786a6fb102af0e.png" alt="img"></p>
<p>拉普拉斯滤波器卷积核表示</p>
<p><img src="https://upload-images.jianshu.io/upload_images/17221499-e20d129fc03194b2.png" alt="img"></p>
<p><strong>利用Laplacian滤波器实现图像的锐化</strong></p>
<p>​    <em><strong>由于拉普拉斯是一种微分算子，它的应用可增强图像中灰度突变的区域，减弱灰度的缓慢变化区域。</strong></em></p>
<p>​    因此，锐化处理可选择拉普拉斯算子对原图像进行处理，产生描述灰度突变的图像，再将拉普拉斯图像与原始图像叠加而产生锐化图像：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/17221499-4d9aa12dca91d82e.png" alt="img"></p>
<p>使用拉普拉斯滤波器实现的图像锐化算法 ↑</p>
<p>​    其中，f(x,y)为原始图像，g(x,y)为锐化后图像，c为-1（卷积核中间为负数时，若卷积核中间为正数，则c为1）</p>
<p><img src="https://upload-images.jianshu.io/upload_images/17221499-d07f876233acc8f8.jpg" alt="img"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/17221499-d637f9f3eed4ac8f.jpg" alt="img"></p>
]]></content>
  </entry>
  <entry>
    <title>Lucas-Kanade 追踪算法</title>
    <url>/2021/05/17/Lucas-Kanade-%E8%BF%BD%E8%B8%AA%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/u012554092/article/details/78128795">https://blog.csdn.net/u012554092/article/details/78128795</a></p>
<p><a href="https://blog.csdn.net/u014568921/article/details/46638557">https://blog.csdn.net/u014568921/article/details/46638557</a></p>
<p>算法实现：</p>
<p><a href="https://blog.csdn.net/zhzhji440/article/details/43866695">https://blog.csdn.net/zhzhji440/article/details/43866695</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>MVC模式</title>
    <url>/2021/04/23/MVC%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<p>MVC 模式代表 Model-View-Controller（模型-视图-控制器） 模式。这种模式用于应用程序的分层开发。</p>
<ul>
<li><p><strong>Model（模型）</strong> - 模型代表一个存取数据的对象或 JAVA POJO。它也可以带有逻辑，在数据变化时更新控制器。模型表示企业数据和业务规则。在MVC的三个部件中，模型拥有最多的处理任务。例如它可能用像<a href="https://baike.baidu.com/item/EJB">EJB</a>s和ColdFusion Components这样的构件对象来处理数据库，被模型返回的数据是中立的，就是说模型与数据格式无关，这样一个模型能为多个视图提供数据，由于应用于模型的代码只需写一次就可以被多个视图重用，所以减少了代码的重复性。</p>
</li>
<li><p><strong>View（视图）</strong> - 视图代表模型包含的数据的可视化。</p>
<p>视图是用户看到并与之交互的界面。对老式的Web应用程序来说，视图就是由<a href="https://baike.baidu.com/item/HTML">HTML</a>元素组成的界面，在新式的Web应用程序中，<a href="https://baike.baidu.com/item/HTML">HTML</a>依旧在视图中扮演着重要的角色，但一些新的技术已层出不穷，它们包括[Adobe Flash](<a href="https://baike.baidu.com/item/Adobe">https://baike.baidu.com/item/Adobe</a> Flash)和像<a href="https://baike.baidu.com/item/XHTML">XHTML</a>，<a href="https://baike.baidu.com/item/XML">XML</a>/<a href="https://baike.baidu.com/item/XSL">XSL</a>,<a href="https://baike.baidu.com/item/WML">WML</a>等一些标识语言和[Web services](<a href="https://baike.baidu.com/item/Web">https://baike.baidu.com/item/Web</a> services).</p>
<p>MVC好处是它能为应用程序处理很多不同的<a href="https://baike.baidu.com/item/%E8%A7%86%E5%9B%BE">视图</a>。在视图中其实没有真正的处理发生，不管这些数据是联机存储的还是一个雇员列表，作为视图来讲，它只是作为一种输出数据并允许用户操纵的方式。</p>
</li>
<li><p><strong>Controller（控制器）</strong> - 控制器作用于模型和视图上。它控制数据流向模型对象，并在数据变化时更新视图。它使视图与模型分离开。控制器接受用户的输入并调用模型和视图去完成用户的需求，所以当单击Web页面中的超链接和发送<a href="https://baike.baidu.com/item/HTML%E8%A1%A8%E5%8D%95">HTML表单</a>时，控制器本身不输出任何东西和做任何处理。它只是接收请求并决定调用哪个模型构件去处理请求，然后再确定用哪个视图来显示返回的数据。</p>
</li>
</ul>
<p><img src="https://images2015.cnblogs.com/blog/811883/201704/811883-20170423150024694-1439613206.jpg" alt="img"></p>
<p><img src="https://images2015.cnblogs.com/blog/811883/201704/811883-20170423150019101-1710764799.jpg" alt="img"></p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>Statement和PrepareStatement的区别详解</title>
    <url>/2021/04/21/Statement%E5%92%8CPrepareStatement%E7%9A%84%E5%8C%BA%E5%88%AB%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>联系：<br>1.PreparedStatement继承自Statement，两者都是接口。<br>2.内部都要建立类似于Sockt连接，效率都不是特别高。</p>
<p>从资源利用和安全的角度区分两者的不同：</p>
<p>关于批处理时如何选择，以数据量大小位标准分三种情况：<br>1）量比较小，二者皆差别不大。<br>2）量比较多，在PreparedStatement预编译空间范围之内，选择PreparedStatement，因为其只预编译一次sql语句。<br>3）量特别大，使用Statement，因为PrepareStatement的预编译空间有限，当数据量特别大时，会发生异常。<br>特殊情况还有：执行不同的sql语句，批处理…等等 ，可以从消耗资源的角度再次分析。</p>
<p>批处理综合分析：<br>使用PreparedStatement代码演示：<br>在批处理过程中包含的sql语句的主干部分(sql语句)必须相同，改变的只是参数，因此编译一次sql语句即可，极大提高性能，但其主干必须相同则影响了灵活性。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"> 2         Connection conn = null;</span><br><span class="line"> 3         Statement stat = null;</span><br><span class="line"> 4         PreparedStatement ps = null;</span><br><span class="line"> 5         </span><br><span class="line"> 6         try &#123;</span><br><span class="line"> 7             Class.forName(&quot;com.mysql.jdbc.Driver&quot;);</span><br><span class="line"> 8             conn = DriverManager.getConnection(&quot;jdbc:mysql:///mydb5&quot;,&quot;root&quot;,&quot;admin&quot;);</span><br><span class="line"> 9             //开始事务</span><br><span class="line">10             conn.setAutoCommit(false);</span><br><span class="line">11             String sql = &quot;insert into tb_batch values (null,?)&quot;;</span><br><span class="line">12             ps = conn.prepareStatement(sql);</span><br><span class="line">13             for(int i=2000;i&lt;3000;i++)&#123;</span><br><span class="line">14                 ps.setString(1, &quot;tong&quot;+i);</span><br><span class="line">15                 ps.addBatch();</span><br><span class="line">16             &#125;</span><br><span class="line">17             ps.executeBatch();</span><br><span class="line">18             //提交事务</span><br><span class="line">19             conn.commit();</span><br><span class="line">22         &#125; catch (Exception e) &#123;</span><br><span class="line">23             e.printStackTrace();</span><br><span class="line">24         &#125;finally&#123;</span><br><span class="line">25             JDBCutils.closeResou(conn, ps, null);</span><br><span class="line">26         &#125;     </span><br><span class="line">29     &#125;</span><br></pre></td></tr></table></figure>

<p>使用Statement代码演示：<br>可以包含结构不同的sql语句，灵活性得到提高，但没有预编译机制, 效率低下；并且你会发现发送的sql语句主干部分相同，只是参数不同， 但是主干部分每次都需要重复写入，极为麻烦。.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Connection conn = null;</span><br><span class="line"> 4         Statement stat = null;</span><br><span class="line"> 5         //注册驱动和连接数据库</span><br><span class="line"> 6         conn = JDBCutils.getConn();</span><br><span class="line"> 7         try &#123;</span><br><span class="line"> 8             stat = conn.createStatement();</span><br><span class="line"> 9             stat.addBatch(&quot;drop database if exists mydb5&quot;);</span><br><span class="line">10             stat.addBatch(&quot;create database mydb5&quot;);</span><br><span class="line">13             stat.addBatch(&quot;insert into tb_batch values(null,&#x27;a&#x27;)&quot;);</span><br><span class="line">14             stat.addBatch(&quot;insert into tb_batch values(null,&#x27;bbb&#x27;)&quot;);</span><br><span class="line">15             stat.addBatch(&quot;insert into tb_batch values(null,&#x27;cccccc&#x27;)&quot;);</span><br><span class="line">16             stat.executeBatch();     </span><br><span class="line">19         &#125; catch (Exception e) &#123;</span><br><span class="line">20             e.printStackTrace();</span><br><span class="line">21         &#125;finally&#123;</span><br><span class="line">22             JDBCutils.closeResou(conn, stat, null);</span><br><span class="line">23         &#125;</span><br><span class="line">24         </span><br></pre></td></tr></table></figure>





<p><strong>从数据库安全的角度：</strong><br>PreparedStatement可防止SQL注入。SQL注入情况如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//输入要删除的账户：&#x27;abc&#x27; or 1=1</span><br><span class="line">String username=&quot;&#x27;abc&#x27; or 1=1&quot;</span><br><span class="line"> </span><br><span class="line">//等待用户输入的SQL语句</span><br><span class="line">String sql=select * from user where username =&#x27;&quot;+username+&quot;&#x27;;</span><br></pre></td></tr></table></figure>



<p>可以发现输入密码时，已经属于非法输入<br>使用Statement 的话最后的sql语句则是如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from user where name = &#x27;abc&#x27; or 1=1;</span><br></pre></td></tr></table></figure>



<p>数据库容易被人恶意全部删除。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title>What&#39;s this website</title>
    <url>/2021/04/17/What-s-this-website/</url>
    <content><![CDATA[<p>此网站仅为学习过程中的笔记，供大家分享。</p>
<p><strong>一緒に頑張りましょう！</strong></p>
<p><strong>What I’m doing right now, I’m chasing perfection.</strong></p>
<p>如有疑问请发邮件到 <a href="mailto:&#x31;&#x33;&#x30;&#49;&#x34;&#x32;&#48;&#52;&#50;&#55;&#64;&#113;&#x71;&#46;&#99;&#111;&#x6d;">&#x31;&#x33;&#x30;&#49;&#x34;&#x32;&#48;&#52;&#50;&#55;&#64;&#113;&#x71;&#46;&#99;&#111;&#x6d;</a></p>
<p><img src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=1872327711,3275977009&fm=26&gp=0.jpg" alt="img"></p>
]]></content>
  </entry>
  <entry>
    <title>active learning</title>
    <url>/2021/09/19/active-learning/</url>
    <content><![CDATA[<p>主动学习（Active Learning）的大致思路就是：通过机器学习的方法获取到那些比较<strong>“难”</strong>分类的样本数据，让人工再次确认和审核，然后将人工标注得到的数据再次使用有监督学习模型或者半监督学习模型进行训练，逐步提升模型的效果，将人工经验融入机器学习的模型中。</p>
<p>那么主动学习（Active Learning）的整体思路究竟是怎样的呢？在机器学习的建模过程中，通常包括样本选择，模型训练，模型预测，模型更新这几个步骤。在主动学习这个领域则需要把标注候选集提取和人工标注这两个步骤加入整体流程，也就是：</p>
<ol>
<li>机器学习模型：包括机器学习模型的训练和预测两部分；</li>
<li>待标注的数据候选集提取：依赖主动学习中的查询函数（Query Function）；</li>
<li>人工标注：专家经验或者业务经验的提炼；</li>
<li>获得候选集的标注数据：获得更有价值的样本数据；</li>
<li>机器学习模型的更新：通过增量学习或者重新学习的方式更新模型，从而将人工标注的数据融入机器学习模型中，提升模型效果</li>
</ol>
<p><img src="https://img2018.cnblogs.com/blog/532548/201905/532548-20190527214307519-1416709660.png" alt="img"></p>
<p>其中 L 是用于训练已标注的样本；</p>
<p>C 为一组或者一个算法模型，用户接收上一轮的标记样本集，通过负反馈调整模型参数，并输出对应的预测结果向量集；</p>
<p>Q 是查询函数，用于从当前剩余的未标注样本池（未标记样本会逐渐减少）U 中查询信息量最大（最不确定）的top样本；</p>
<p>S是督导者，可以为 U 中样本标注正确的标签；</p>
<p>active learning模型**通过少量初始标记样本 L 开始学习，通过一定的<font color='red'>查询函数 Q 选择出一个或一批最有用的样本</font>**，并向督导者询问标签，然后利用获得的新知识来训练分类器和进行下一轮查询。主动学习是一个循环的过程，直至达到某一停止准则为止。</p>
<p>需要注意的是，active learning是一个算法框架，上图中的单个模块具备可替换性（alternative）</p>
<p><a href="https://zhuanlan.zhihu.com/p/239756522">https://zhuanlan.zhihu.com/p/239756522</a></p>
]]></content>
  </entry>
  <entry>
    <title>cluster聚合层次聚类算法</title>
    <url>/2021/11/05/cluster%E8%81%9A%E5%90%88%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p><img src="https://pic4.zhimg.com/v2-05343ed3ff48cec83244ab64760acdeb_r.jpg" alt="img"></p>
<p>层次聚类通过对数据集在不同层次进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合（agglomerative）策略，也可采用“自顶向下”的分拆（divisive）策略。“自底而上”的算法开始时把每一个原始数据看作一个单一的聚类簇，然后不断聚合小的聚类簇成为大的聚类。“自顶向下”的算法开始把所有数据看作一个聚类，通过不断分割大的聚类直到每一个单一的数据都被划分。</p>
<p>根据聚类簇之间距离的计算方法的不同，层次聚类算法可以大致分为：单链接（Single-link）算法，全链接算法（complete-link）或均链接算法（average-link）。单链接算法用两个聚类簇中最近的样本距离作为两个簇之间的距离；而全链接使用计算两个聚类簇中最远的样本距离；均链接算法中两个聚类之间的距离由两个簇中所有的样本共同决定。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/f097aafc-3580-47c6-bb5e-4e4bed04aaa5/1523979971355.jpg" alt="img"></p>
<p>“自底向上”的聚合层次聚类算法</p>
<p>（1）计算任意两个数据之间的距离得到一个相似度矩阵（proximity matrix），并把每一个单一的数据看作是一个聚类；</p>
<p>（2）查找相似度矩阵中距离最小的两个聚类，把他们聚合为一个新的聚类，然后根据这个新的聚类重新计算相似度矩阵；</p>
<p>（3）重复（2）直到所有的数据都被归入到一个聚类中。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/ebee159c-0067-4ecc-af7a-2559c9e5f781/1523979971408.jpg" alt="img"></p>
<p>“自顶向下”的分拆（divisive）策略则与之相反。</p>
<p>可以使用单链接或者全链接的的方式来聚类簇之间的距离，会使得聚类的最终结果有所不同。</p>
<p>针对层次聚类不适于解决大量数据的问题，Zhang 等人提出了一种基于聚类特征树（Clustering Feature Tree）的层次聚类算法：BIRCH算法（Balanced Iterative Reducing and Clustering Using Hierarchies）。BIRCH算法实现了只需要一次对数据集的遍历就可以完成聚类。其聚类的主要过程就是将所有数据依次插入构建聚类特征树的过程，而树上的每一个节点所包含的数据构成了一个对应的聚类簇。同时对于包含数据极少的节点我们可以认为它是异常点从而进行去除。</p>
<p>层次聚类的另外一个变种算法是名为变色龙算法（Chameleon）。它是由George Karypis 教授提出的一种基于动态建模（Dynamic modeling）思想的层次聚类算法。该算法通过计算两个聚类簇之间的相似度和互连性从而克服CURE和ROCK等算法的缺点。变色龙算法首先将所有的数据根据他们之间的距离划分成众多的小的簇，通过计算相近簇之间的相似度和互连性不断合并形成大的聚类簇。</p>
<p>k-means</p>
<p>注意不要混淆K-近邻算法和K均值聚类。 K-近邻算法是一种分类算法，也是监督学习的一个子集。 K-means是一种聚类算法，也是非监督学习的一个子集。</p>
<p><a href="https://zhuanlan.zhihu.com/p/104355127">https://zhuanlan.zhihu.com/p/104355127</a></p>
]]></content>
  </entry>
  <entry>
    <title>图像校正(Image Rectification)</title>
    <url>/2021/05/19/%E5%9B%BE%E5%83%8F%E6%A0%A1%E6%AD%A3-Image-Rectification/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/LoseInVain/article/details/102775734">https://blog.csdn.net/LoseInVain/article/details/102775734</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>k-means 如何确定k 和 初始聚类</title>
    <url>/2021/11/09/%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9Ak%E5%9C%A8k-means/</url>
    <content><![CDATA[<p><strong>K****值确定</strong></p>
<p>**法1：(轮廓系数)**在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。</p>
<p><strong>法2：(Calinski-Harabasz准则)</strong></p>
<p>  <img src="https://images2015.cnblogs.com/blog/804917/201609/804917-20160904180009662-1327263953.png" alt="img"></p>
<p>其中SSB是类间方差，<img src="https://images2015.cnblogs.com/blog/804917/201609/804917-20160904180019100-58882840.png" alt="img"> ，m为所有点的中心点,mi为某类的中心点；</p>
<p>SSW是类内方差，<img src="https://images2015.cnblogs.com/blog/804917/201609/804917-20160904180044867-864335001.png" alt="img">；</p>
<p>(N-k)/(k-1)是复杂度；</p>
<p><img src="https://images2015.cnblogs.com/blog/804917/201609/804917-20160904180103509-48249157.png" alt="img">比率越大，数据分离度越大.</p>
<p>z常见的一种方法是elbow method，x轴为聚类的数量，y轴为WSS（within cluster sum of squares）也就是各个点到cluster中心的距离的平方的和。</p>
<p><img src="https://pic1.zhimg.com/v2-916dd08810a3e3ca23ecab29378cbfdc_r.jpg?source=1940ef5c" alt="preview"></p>
<p><strong>初始点选择方法：</strong></p>
<p>思想，初始的聚类中心之间相互距离尽可能远.</p>
<p><strong>法1(kmeans++):</strong></p>
<p>1、从输入的数据点集合中随机选择一个点作为第一个聚类中心</p>
<p>2、对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</p>
<p>3、选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</p>
<p>4、重复2和3直到k个聚类中心被选出来</p>
<p>5、利用这k个初始的聚类中心来运行标准的k-means算法</p>
<p> 从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下：</p>
<p>1、先从我们的数据库随机挑个随机点当“种子点”</p>
<p>2、对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。</p>
<p>3、然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。</p>
<p>4、重复2和3直到k个聚类中心被选出来</p>
<p>5、利用这k个初始的聚类中心来运行标准的k-means算法</p>
<p><strong>法2：</strong>选用层次聚类或Canopy算法进行初始聚类，然后从k个类别中分别随机选取k个点</p>
<p>，来作为kmeans的初始聚类中心点</p>
<p><strong>优点：</strong></p>
<p>1、 算法快速、简单;</p>
<p>2、 容易解释</p>
<p>3、 聚类效果中上</p>
<p>4、 适用于高维</p>
<p><strong>缺陷：</strong></p>
<p>1、 对离群点敏感，对噪声点和孤立点很敏感(通过k-centers算法可以解决)</p>
<p>2、 K-means算法中聚类个数k的初始化</p>
<p>3、初始聚类中心的选择，不同的初始点选择可能导致完全不同的聚类结果。</p>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title>对极几何</title>
    <url>/2021/05/17/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/clarenceliang/p/6704970.html">https://www.cnblogs.com/clarenceliang/p/6704970.html</a></p>
<p><a href="https://blog.csdn.net/tina_ttl/article/details/52749542">https://blog.csdn.net/tina_ttl/article/details/52749542</a></p>
<p><a href="http://www.360doc.com/content/14/0205/14/10724725_349963695.shtml">http://www.360doc.com/content/14/0205/14/10724725_349963695.shtml</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/79845576">https://zhuanlan.zhihu.com/p/79845576</a></p>
<p><a href="https://blog.csdn.net/ssw_1990/article/details/53355572">https://blog.csdn.net/ssw_1990/article/details/53355572</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>快速补全代码</title>
    <url>/2021/04/22/%E5%BF%AB%E9%80%9F%E8%A1%A5%E5%85%A8%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p><strong>将鼠标放在接口的名称处按 Alt+Enter 键</strong></p>
<p>实现接口：<strong>鼠标左键单击：Implement interface ，进入创建实现类的名字以及指明其所在的包的界面。选择好之后，点击ok即可完成</strong></p>
]]></content>
  </entry>
  <entry>
    <title>摄像机原理</title>
    <url>/2021/04/22/%E6%91%84%E5%83%8F%E6%9C%BA%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>The first important content is the <strong>image forming process.</strong> The image is an array. There is only one array for black and white images, and there are three arrays for color images. Then the picture is recorded by the camera because of the black box camera and the principle of small hole forming. There is a perspective effect through the camera, and the parallel lines will converge at a vanishing point in the distance in the picture. <strong>Homogeneous coordinates</strong> represent a vector that is originally n-dimensional with an n+1-dimensional vector, which refers to a coordinate system used in projection geometry, just like <strong>Cartesian coordinates used in Euclidean geometry.</strong> 2D and 3D transformations include homogeneous coordinates and transformation.</p>
<p><strong>The intrinsic matrix</strong>: The focal length is from the pointer hole to the film, where the unit of focal length in pixels. <strong>The extrinsic matrix</strong> is used to describe the position of the camera in the “world coordinate system” and the direction it points to. The field of view depends on the focal length, the larger the <strong>focal length</strong>, the smaller the field of view. The smaller the focal length, the larger the field of view. The above is the position of the object in the photo, and the next is the pixel size. The biggest function of the <strong>large aperture</strong> is to highlight the subject, blur the background, and create a sense of hierarchy. The small aperture has a large clear range and is suitable for shooting themes such as scenery, travel, and documentary. The scenery is shot with a small aperture. With the pan-focus operation, the entire huge spatial scale picture can be located in a clear range. Finally, the digital camera, using imaging sensors.</p>
<p>The second part is <strong>image processing</strong>, Proposed the <strong>Aliasing problem</strong>, the harm caused by down-sampling danger, Characteristic errors will appear. The solution involves multiple sampling to eliminate frequencies higher than half of the new sampling, Including <strong>Gaussian filtering</strong>.</p>
<p><strong>Tone mapping</strong> makes the Incoming light energy and the Image intensity value not linear.</p>
<p><strong>Bilinear interpolation</strong> is a linear interpolation extension of an interpolation function with two variables. Its core idea is to perform a linear interpolation in two directions. </p>
<p><strong>Chromatic aberration:</strong> Different colors of light have different wavelengths. Therefore, the focal plane of light passing through the lens and light of different wavelengths will be slightly different. Besides, the angles of the chromatic images after passing through the lens will be slightly staggered when they reach the focal plane. That is, the seven basic colors of light reach different focal points. </p>
<p><strong>Histogram Equalization</strong>: It is often used to enhance the overall contrast of the picture, especially for those cases where useful data is represented by similar data. </p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>逻辑回归</title>
    <url>/2021/11/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic Regression 因其简单、可并行化、可解释强深受工业界喜爱。</p>
<p>Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。</p>
<h3 id="1-1-Logistic-分布"><a href="#1-1-Logistic-分布" class="headerlink" title="1.1 Logistic 分布"></a>1.1 Logistic 分布</h3><p>Logistic 分布是一种连续型的概率分布，其<strong>分布函数</strong>和<strong>密度函数</strong>分别为：</p>
<p><img src="https://www.zhihu.com/equation?tex=F(x)+=+P(X+%5Cleq+x)=%5Cfrac%7B1%7D%7B1+e%5E%7B-(x-%5Cmu)/%5Cgamma%7D%7D+%5C%5C+f(x)+=+F%5E%7B%27%7D(X+%5Cleq+x)=%5Cfrac%7Be%5E%7B-(x-%5Cmu)/%5Cgamma%7D%7D%7B%5Cgamma(1+e%5E%7B-(x-%5Cmu)/%5Cgamma%7D)%5E%7B2%7D%7D+%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 表示<strong>位置参数</strong>， <img src="https://www.zhihu.com/equation?tex=%5Cgamma%3E0" alt="[公式]"> 为<strong>形状参数</strong>。我们可以看下其图像特征：</p>
<p><img src="https://pic2.zhimg.com/80/v2-b15289fd1162a807e11949e5396c7989_1440w.jpg" alt="img"></p>
<p>Logistic 分布是由其位置和尺度参数定义的连续分布。Logistic 分布的形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 <img src="https://www.zhihu.com/equation?tex=%5Cmu=0,+%5Cgamma=1" alt="[公式]"> 的特殊形式。</p>
<h3 id="1-2-Logistic-回归"><a href="#1-2-Logistic-回归" class="headerlink" title="1.2 Logistic 回归"></a>1.2 Logistic 回归</h3><p>之前说到 Logistic 回归主要用于分类问题，我们以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。</p>
<p>1.</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTI1MTA5Ni8yMDE3MTAvMTI1MTA5Ni0yMDE3MTAyMDIwMzc1ODI1Ni0xNjA0MzQ3MzgxLnBuZw?x-oss-process=image/format,png" alt="img"></p>
<p>于是我们得到了这样的关系式：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTI1MTA5Ni8yMDE3MTAvMTI1MTA5Ni0yMDE3MTAyMDIxMDE0NzA4NC0xMTk2OTM5MjUxLnBuZw?x-oss-process=image/format,png" alt="img"></p>
<p><img src="https://pic3.zhimg.com/v2-34f3997ae1975cd620a8514e3954fa9e_r.jpg" alt="img"></p>
<p>根据上面得到的关系式，我们可以得到：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTI1MTA5Ni8yMDE3MTAvMTI1MTA5Ni0yMDE3MTAyMDIxMTYyMjExNS0yMDg5OTM1MzMxLnBuZw?x-oss-process=image/format,png" alt="img">　　</p>
<p>而我们再图像上画出得到：</p>
<p>　　<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTI1MTA5Ni8yMDE3MTAvMTI1MTA5Ni0yMDE3MTAyMDIxMTgyNDMwMi0xMTAxMzQ1Mjg4LnBuZw?x-oss-process=image/format,png" alt="img"></p>
<p>这时，直线上方所有样本都是正样本y=1，直线下方所有样本都是负样本y=0。因此我们可以把这条直线成为决策边界。</p>
<p>同理，对于非线性可分的情况，我们只需要引入多项式特征就可以很好的去做分类预测，如下图：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTI1MTA5Ni8yMDE3MTAvMTI1MTA5Ni0yMDE3MTAyMDIxMjk1NTI3MS0xMjIzNTU4Njg2LnBuZw?x-oss-process=image/format,png" alt="img"></p>
<p>　　</p>
<p>值得注意的一点，决策边界并不是训练集的属性，而是假设本身和参数的属性。因为训练集不可以定义决策边界，它只负责拟合参数；而只有参数确定了，决策边界才得以确定。</p>
<p>决策边界可以表示为 <img src="https://www.zhihu.com/equation?tex=w_1x_1+w_2x_2+b=0" alt="[公式]"> ，假设某个样本点 <img src="https://www.zhihu.com/equation?tex=h_w(x)+=+w_1x_1+w_2x_2+b+%3E+0" alt="[公式]"> 那么可以判断它的类别为 1，这个过程其实是感知机。</p>
<p>Logistic 回归还需要加一层，它要找到分类概率 P(Y=1) 与输入向量 x 的直接关系，然后通过比较概率值来判断类别。</p>
<p>考虑二分类问题，给定数据集</p>
<p><img src="https://www.zhihu.com/equation?tex=+D=%7B(x_%7B1%7D,+y_%7B1%7D),(x_%7B2%7D,y_%7B2%7D),%5Ccdots,(x_%7BN%7D,+y_%7BN%7D)%7D,+x_%7Bi%7D+%5Csubseteq+R%5E%7Bn%7D,+y_%7Bi%7D+%5Cin+%7B0,1%7D,i=1,2,%5Ccdots,N+++%5C%5C" alt="[公式]"></p>
<p>考虑到 <img src="https://www.zhihu.com/equation?tex=w%5E%7BT%7Dx+b" alt="[公式]"> 取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率 <img src="https://www.zhihu.com/equation?tex=p(Y=1%7Cx)" alt="[公式]"> ，因为概率的取值也是连续的。</p>
<p>但是对于 <img src="https://www.zhihu.com/equation?tex=w+%5Cne+0" alt="[公式]"> （若等于零向量则没有什么求解的价值）， <img src="https://www.zhihu.com/equation?tex=w%5E%7BT%7Dx+b" alt="[公式]"> 取值为 <img src="https://www.zhihu.com/equation?tex=R" alt="[公式]"> ，不符合概率取值为 0 到 1，因此考虑采用广义线性模型。</p>
<p>最理想的是单位阶跃函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=p(y=1+%7C+x)=%5Cbegin%7Bcases%7D+0,&+z%5Clt+0+%5C%5C+0.5,&+z+=+0%5C%5C+1,&+z%5Cgt+0%5C+%5Cend%7Bcases%7D+,%5Cquad+z=w%5ET+x+b++%5C%5C" alt="[公式]"></p>
<p>但是这个阶跃函数不可微，对数几率函数是一个常用的替代函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=+y+=+%5Cfrac%7B1%7D%7B1+e%5E%7B-(w%5E%7BT%7D+x+++b)%7D%7D++%5C%5C" alt="[公式]"></p>
<p>于是有：</p>
<p><img src="https://www.zhihu.com/equation?tex=+ln+%5Cfrac%7By%7D%7B1%E2%88%92y%7D+=+w%5E%7BT%7Dx+++b+%5C%5C" alt="[公式]"></p>
<p>我们将 y 视为 x 为正例的概率，则 1-y 为 x 为其反例的概率。两者的比值称为<strong>几率（odds）</strong>，指该事件发生与不发生的概率比值，若事件发生的<strong>概率</strong>为 p。则对数几率：</p>
<p><img src="https://www.zhihu.com/equation?tex=+ln(odds)+=+ln+%5Cfrac%7By%7D%7B1%E2%88%92y%7D++%5C%5C" alt="[公式]"></p>
<p>将 y 视为类后验概率估计，重写公式有：</p>
<p><img src="https://www.zhihu.com/equation?tex=w%5E%7BT%7D+x+++b+=+ln%5Cfrac%7BP(Y=1%7Cx)%7D%7B1-P(Y=1%7Cx)%7D+%5C%5C+P(Y=1%7Cx)+=+%5Cfrac%7B1%7D%7B1+e%5E%7B-(w%5E%7BT%7D+x+++b)%7D%7D+%5C%5C" alt="[公式]"></p>
<p>也就是说，输出 Y=1 的对数几率是由输入 x 的<strong>线性函数</strong>表示的模型，这就是<strong>逻辑回归模型</strong>。当 <img src="https://www.zhihu.com/equation?tex=w%5E%7BT%7Dx+b" alt="[公式]"> 的值越接近正无穷， <img src="https://www.zhihu.com/equation?tex=P(Y=1%7Cx)+" alt="[公式]"> 概率值也就越接近 1。因此<strong>逻辑回归的思路</strong>是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p>
<p>在这我们思考个问题，我们使用对数几率的意义在哪？通过上述推导我们可以看到 Logistic 回归实际上是使用线性回归模型的预测值逼近分类任务真实标记的对数几率，其优点有：</p>
<ol>
<li>直接对<strong>分类的概率</strong>建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区别于生成式模型）；</li>
<li>不仅可预测出类别，还能得到该<strong>预测的概率</strong>，这对一些利用概率辅助决策的任务很有用；</li>
<li>对数几率函数是<strong>任意阶可导的凸函数</strong>，有许多数值优化算法都可以求出最优解。</li>
</ol>
<h3 id="1-3-代价函数"><a href="#1-3-代价函数" class="headerlink" title="1.3 代价函数"></a>1.3 代价函数</h3><p>逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。</p>
<p>设：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P(Y=1%7Cx)+&=+p(x)+%5C%5C++P(Y=0%7Cx)+&=+1-+p(x)+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"></p>
<p>似然函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=L(w)=%5Cprod%5Bp(x_%7Bi%7D)%5D%5E%7By_%7Bi%7D%7D%5B1-p(x_%7Bi%7D)%5D%5E%7B1-y_%7Bi%7D%7D++%5C%5C" alt="[公式]"></p>
<p>为了更方便求解，我们对等式两边同取对数，写成对数似然函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7Baligned%7D+L(w)&=%5Csum%5By_%7Bi%7Dlnp(x_%7Bi%7D)+(1-y_%7Bi%7D)ln(1-p(x_%7Bi%7D))%5D+%5C%5C+&=%5Csum%5By_%7Bi%7Dln%5Cfrac%7Bp(x_%7Bi%7D)%7D%7B1-p(x_%7Bi%7D)%7D+ln(1-p(x_%7Bi%7D))%5D++%5C%5C+&=%5Csum%5By_%7Bi%7D(w+%5Ccdot+x_%7Bi%7D)+-+ln(1+e%5E%7Bw+%5Ccdot+x_%7Bi%7D%7D)%5D+%5Cend%7Baligned%7D+%5C%5C" alt="[公式]"></p>
<p>在机器学习中我们有损失函数的概念，其衡量的是模型预测错误的程度。如果取整个数据集上的平均对数似然损失，我们可以得到:</p>
<p><img src="https://www.zhihu.com/equation?tex=J(w)=-%5Cfrac%7B1%7D%7BN%7DlnL(w)+%5C%5C" alt="[公式]"></p>
<p>即在逻辑回归模型中，我们<strong>最大化似然函数</strong>和<strong>最小化损失函数</strong>实际上是等价的。</p>
<h3 id="1-4-求解"><a href="#1-4-求解" class="headerlink" title="1.4 求解"></a>1.4 求解</h3><p>求解逻辑回归的方法有非常多，我们这里主要聊下梯度下降和牛顿法。优化的主要目标是找到一个方向，参数朝这个方向移动之后使得损失函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。逻辑回归的损失函数是：</p>
<p><img src="https://www.zhihu.com/equation?tex=J(w)+=++-%5Cfrac%7B1%7D%7Bn%7D(%5Csum_%7Bi=1%7D%5En(y_ilnp(x_i)+(1-y_i)ln(1-p(x_i)))+%5C%5C" alt="[公式]"></p>
<ol>
<li>随机梯度下降</li>
</ol>
<p>梯度下降是通过 J(w) 对 w 的一阶导数来找下降方向，并且以迭代的方式来更新参数，更新方式为 :</p>
<p><img src="https://www.zhihu.com/equation?tex=+g_i+=+%5Cfrac%7B%5Cpartial+J(w)%7D+%7B%5Cpartial+w_i%7D+=(p(x_i)-y_i)x_i+%5C%5C+w%5E%7Bk+1%7D_i=w%5Ek_i-%5Calpha+g_i" alt="[公式]"></p>
<p>其中 k 为迭代次数。每次更新参数后，可以通过比较 <img src="https://www.zhihu.com/equation?tex=+%7C%7CJ(w%5E%7Bk+1%7D)%E2%88%92J(w%5Ek)%7C%7C+" alt="[公式]"> 小于阈值或者到达最大迭代次数来停止迭代。</p>
<p>\2. 牛顿法</p>
<p>牛顿法的基本思路是，<strong>在现有极小点估计值的附近对 f(x) 做二阶泰勒展开，进而找到极小点的下一个估计值</strong>。假设 <img src="https://www.zhihu.com/equation?tex=w%5Ek" alt="[公式]"> 为当前的极小值估计值，那么有：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cvarphi+(w)+=+J(w%5Ek)+++J%5E%7B%27%7D(w%5Ek)(w-w%5Ek)+%5Cfrac%7B1%7D%7B2%7DJ%5E%7B%22%7D(w%5Ek)(w-w%5Ek)%5E2++%5C%5C" alt="[公式]"></p>
<p>然后令 <img src="https://www.zhihu.com/equation?tex=%CF%86%5E%7B%27%7D(w)=0" alt="[公式]"> ，得到了 <img src="https://www.zhihu.com/equation?tex=+w%5E%7Bk+1%7D=w%5E%7Bk%7D%E2%88%92%5Cfrac%7BJ%5E%7B%27%7D(w%5Ek)%7D%7BJ%5E%7B%22%7D(w%5Ek)%7D" alt="[公式]"> 。因此有迭代更新式：</p>
<p><img src="https://www.zhihu.com/equation?tex=w%5E%7Bk+1%7D+=+w%5E%7Bk%7D+-+%5Cfrac%7BJ%5E%7B%27%7D(w%5E%7Bk%7D)%7D%7BJ%5E%7B%22%7D(w%5E%7Bk%7D)%7D+=+w%5E%7Bk%7D+-+H_%7Bk%7D%5E%7B-1%7D%5Ccdot+g_%7Bk%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=H_%7Bk%7D%5E%7B-1%7D" alt="[公式]"> 为海森矩阵：</p>
<p><img src="https://www.zhihu.com/equation?tex=H_%7Bmn%7D+=+%5Cfrac+%7B%5Cpartial%5E2+J(w)%7D+%7B%5Cpartial+w_%7Bm%7D+%5Cpartial+w_%7Bn%7D%7D+=h_%7Bw%7D(x%5E%7B(i)%7D)(1-p_%7Bw%7D(x%5E%7B(i)%7D))x%5E%7B(i)%7D_mx%5E%7B(i)%7D_n++%5C%5C" alt="[公式]"></p>
<p>此外，这个方法需要目标函数是二阶连续可微的，本文中的 J(w) 是符合要求的。</p>
<h3 id="1-5-正则化"><a href="#1-5-正则化" class="headerlink" title="1.5 正则化"></a>1.5 正则化</h3><p>正则化是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。</p>
<p>在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。</p>
<p>正则化一般会采用 L1 范式或者 L2 范式，其形式分别为 <img src="https://www.zhihu.com/equation?tex=%CE%A6(w)=%7C%7Cx%7C%7C_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%CE%A6(w)=%7C%7Cx%7C%7C_2+" alt="[公式]"> 。</p>
<ol>
<li>L1 正则化</li>
</ol>
<p>LASSO 回归，相当于为模型添加了这样一个先验知识：w 服从零均值拉普拉斯分布。 首先看看拉普拉斯分布长什么样子：</p>
<p><img src="https://www.zhihu.com/equation?tex=f(w%7C%5Cmu,b)=%5Cfrac%7B1%7D%7B2b%7D%5Cexp+%5Cleft+(+-%5Cfrac%7B%7Cw-%5Cmu%7C%7D%7Bb%7D%5Cright+)%5C%5C" alt="[公式]"></p>
<p>由于引入了先验知识，所以似然函数这样写：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7Baligned%7D+L(w)&=P(y%7Cw,x)P(w)%5C%5C+&=%5Cprod_%7Bi=1%7D%5ENp(x_i)%5E%7By_i%7D(1-p(x_i))%5E%7B1-y_i%7D%5Cprod_%7Bj=1%7D%5Ed+%5Cfrac%7B1%7D%7B2b%7D%5Cexp+%5Cleft+(%7B-%5Cfrac%7B%7Cw_j%7C%7D%7Bb%7D%7D+%5Cright+)+%5Cend%7Baligned%7D++%5C%5C" alt="[公式]"></p>
<p>取 log 再取负，得到目标函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cln+L(w)=-%5Csum_i+%5By_i%5Cln+p(x_i)+(1-y_i)ln(1-p(x_i))%5D+%5Cfrac%7B1%7D%7B2b%5E2%7D%5Csum_j%7Cw_j%7C++%5C%5C" alt="[公式]"></p>
<p>等价于原始损失函数的后面加上了 L1 正则，因此 L1 正则的本质其实是为模型增加了“<strong>模型参数服从零均值拉普拉斯分布</strong>”这一先验知识。</p>
<p>\2. L2 正则化</p>
<p>Ridge 回归，相当于为模型添加了这样一个先验知识：w 服从零均值正态分布。</p>
<p>首先看看正态分布长什么样子：</p>
<p><img src="https://www.zhihu.com/equation?tex=f(w%7C%5Cmu,%5Csigma)=%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%5Cexp+%5Cleft+(+-%5Cfrac%7B(w-%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D%5Cright+)++%5C%5C" alt="[公式]"></p>
<p>由于引入了先验知识，所以似然函数这样写：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+L(w)&=P(y%7Cw,x)P(w)%5C%5C+&=%5Cprod_%7Bi=1%7D%5ENp(x_i)%5E%7By_i%7D(1-p(x_i))%5E%7B1-y_i%7D%5Cprod_%7Bj=1%7D%5Ed+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%5Cexp+%5Cleft+(%7B-%5Cfrac%7Bw_j%5E2%7D%7B2%5Csigma%5E2%7D%7D+%5Cright+)%5C%5C+&=%5Cprod_%7Bi=1%7D%5ENp(x_i)%5E%7By_i%7D(1-p(x_i))%5E%7B1-y_i%7D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%5Cexp+%5Cleft+(%7B-%5Cfrac%7Bw%5ETw%7D%7B2%5Csigma%5E2%7D%7D+%5Cright+)+%5Cend%7Baligned%7D+%5C%5C" alt="[公式]"></p>
<p>取 ln 再取负，得到目标函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cln+L(w)=-%5Csum_i+%5By_i%5Cln+p(x_i)+(1-y_i)ln(1-p(x_i))%5D+%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7Dw%5ETw+%5C%5C" alt="[公式]"></p>
<p>等价于原始的损失函数后面加上了 L2 正则，因此 L2 正则的本质其实是为模型增加了“<strong>模型参数服从零均值正态分布</strong>”这一先验知识。</p>
<p>\3. L1 和 L2 的区别</p>
<p>从上面的分析中我们可以看到，L1 正则化增加了所有权重 w 参数的绝对值之和逼迫更多 w 为零，也就是变稀疏（ L2 因为其导数也趋 0, 奔向零的速度不如 L1 给力了）。我们对稀疏规则趋之若鹜的一个关键原因在于它能<strong>实现特征的自动选择</strong>。一般来说，大部分特征 x_i 都是和最终的输出 y_i 没有关系或者不提供任何信息的。在最小化目标函数的时候考虑 x_i 这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确 y_i 的预测。L1 正则化的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为 0。</p>
<p>L2 正则化中增加所有权重 w 参数的平方之和，逼迫所有 w 尽可能趋向零但不为零（L2 的导数趋于零）。因为在未加入 L2 正则化发生过拟合时，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化很剧烈，也就是某些 w 值非常大。为此，L2 正则化的加入就惩罚了权重变大的趋势。</p>
<p>我们以二维样本为例，图解阐述加入 L1 正则化和 L2 正则化之后目标函数求解时发生的变化。</p>
<ul>
<li>原函数曲线等高线（同颜色曲线上，每一组 <img src="https://www.zhihu.com/equation?tex=w_1,w_2" alt="[公式]"> 带入后值都相同)</li>
</ul>
<p><img src="https://pic1.zhimg.com/v2-896a01ead6aee864250941a64e7931e4_r.jpg" alt="img"></p>
<p>那现在我们看下加了 L1 正则化和 L2 正则化之后，目标函数求解的时候，最终解会有什么变化。</p>
<p><img src="https://pic4.zhimg.com/v2-91986c70dab4d152339ea085321c6f3f_r.jpg" alt="img"></p>
<p>从上边两幅图中我们可以看出：</p>
<ul>
<li>如果不加 L1 和 L2 正则化的时候，对于线性回归这种目标函数凸函数的话，我们最终的结果就是最里边的紫色的小圈圈等高线上的点。</li>
<li>当加入 L1 正则化的时候，我们先画出 <img src="https://www.zhihu.com/equation?tex=%7Cw_1%7C+%7Cw_2%7C=F+" alt="[公式]"> 的图像，也就是一个菱形，代表这些曲线上的点算出来的 <img src="https://www.zhihu.com/equation?tex=L_1" alt="[公式]"> 范数 <img src="https://www.zhihu.com/equation?tex=%7Cw_1%7C+%7Cw_2%7C+" alt="[公式]"> 都为 F。那我们现在的目标是不仅是原曲线算的值要小（越来越接近中心的紫色圈圈），还要使得这个菱形越小越好（F 越小越好）。那么还和原来一样的话，过中心紫色圈圈的那个菱形明显很大，因此我们要取到一个恰好的值。那么如何求值呢？</li>
</ul>
<p><img src="https://pic4.zhimg.com/v2-efc752bd6d1ce09dbf2e18b9766570eb_r.jpg" alt="img"></p>
<ol>
<li>以同一条原曲线目标等高线来说，现在以最外圈的红色等高线为例，我们看到，对于红色曲线上的每个点都可做一个菱形，根据上图可知，当这个菱形与某条等高线相切（仅有一个交点）的时候，这个菱形最小，上图相割对比较大的两个菱形对应的 L1 范数更大。用公式说这个时候能使得在相同的 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D++%5Csum_%7Bi+=+1%7D%5EN%7B(y_i+-w%5ET+x_i)%5E2+%7D" alt="[公式]"> ，由于相切的时候的 <img src="https://www.zhihu.com/equation?tex=C%7C%7Cw%7C%7C_%7B1%7D+" alt="[公式]"> 小，即 <img src="https://www.zhihu.com/equation?tex=%7Cw_1%7C+%7Cw_2%7C" alt="[公式]">所以能够使得<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D++%5Csum%7Bi+=+1%7D%5EN%7B(y_i+-w%5ET+x_i)%5E2+%7D++C%7C%7Cw%7C%7C_%7B1%7D" alt="[公式]"> 更小；</li>
<li>有了第一条的说明我们可以看出，最终加入 L1 范数得到的解一定是某个菱形和某条原函数等高线的切点。现在有个比较重要的结论来了，<strong>我们经过观察可以看到，几乎对于很多原函数等高曲线，和某个菱形相交的时候及其容易相交在坐标轴（比如上图），也就是说最终的结果，解的某些维度及其容易是 0，比如上图最终解是</strong><img src="https://www.zhihu.com/equation?tex=w=(0,x)" alt="[公式]"><strong>，这也就是我们所说的 L1 更容易得到稀疏解（解向量中 0 比较多）的原因；</strong></li>
<li>当然光看着图说，L1 的菱形更容易和等高线相交在坐标轴一点都没说服力，只是个感性的认识，我们接下来从更严谨的方式来证明，简而言之就是假设现在我们是一维的情况下 <img src="https://www.zhihu.com/equation?tex=h(w)+=+f(w)+++C%7Cw%7C" alt="[公式]"> ，其中 h(w) 是目标函数， <img src="https://www.zhihu.com/equation?tex=f(w)+" alt="[公式]"> 是没加 L1 正则化项前的目标函数， <img src="https://www.zhihu.com/equation?tex=C%7Cw%7C" alt="[公式]"> 是 L1 正则项，要使得 0 点成为最值可能的点，虽然在 0 点不可导，但是我们只需要让 0 点左右的导数异号，即 <img src="https://www.zhihu.com/equation?tex=h_%7Bl%7D%5E%7B%27%7D(0)++h_%7Br%7D%5E%7B%27%7D(0)+=+(f%5E%7B%27%7D(0)+++C)(f%5E%7B%27%7D(0)+-+C)+%3C+0+" alt="[公式]"> 即可也就是 <img src="https://www.zhihu.com/equation?tex=+C+%3E%7Cf%5E%7B%27%7D(0)%7C" alt="[公式]"> 的情况下，0 点都是可能的最值点。</li>
</ol>
<p>当加入 L2 正则化的时候，分析和 L1 正则化是类似的，也就是说我们仅仅是从菱形变成了圆形而已，同样还是求原曲线和圆形的切点作为最终解。当然与 L1 范数比，我们这样求的 L2 范数的<strong>从图上来看，不容易交在坐标轴上，但是仍然比较靠近坐标轴。因此这也就是我们老说的，L2 范数能让解比较小（靠近 0），但是比较平滑（不等于 0）。</strong></p>
<p>综上所述，我们可以看见，加入正则化项，在最小化经验误差的情况下，可以让我们选择解更简单（趋向于 0）的解。</p>
<p>结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。</p>
<p><strong>因此，加正则化项就是结构风险最小化的一种实现。</strong></p>
<p><strong>正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。</strong></p>
<p><strong>简单总结下</strong>：</p>
<p>给 loss function 加上正则化项，能使新得到的优化目标函数 <img src="https://www.zhihu.com/equation?tex=h+=+f+%7C%7Cw%7C%7C+" alt="[公式]"> ，需要在 f 和 ||w|| 中做一个权衡，如果还像原来只优化 f 的情况下，那可能得到一组解比较复杂，使得正则项 ||w|| 比较大，那么 h 就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。</p>
<p>L1 正则化就是在 loss function 后边所加正则项为 L1 范数，加上 L1 范数容易得到稀疏解（0 比较多）。L2 正则化就是 loss function 后边所加正则项为 L2 范数的平方，加上 L2 正则相比于 L1 正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于 0（但不是等于 0，所以相对平滑）的维度比较多，降低模型的复杂度。</p>
<h3 id="1-6-并行化"><a href="#1-6-并行化" class="headerlink" title="1.6 并行化"></a>1.6 并行化</h3><p>从逻辑回归的求解方法中我们可以看到，无论是随机梯度下降还是牛顿法，或者是没有提到的拟牛顿法，都是需要计算梯度的，因此逻辑回归的并行化最主要的就是对目标函数梯度计算的并行化。</p>
<p>我们看到目标函数的梯度向量计算中只需要进行向量间的点乘和相加，可以很容易将每个迭代过程拆分成相互独立的计算步骤，由不同的节点进行独立计算，然后归并计算结果。</p>
<p>下图是一个标签和样本矩阵，行为特征向量，列为特征维度。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9c5aea83687172eb7d4756397bc2669e_1440w.jpg" alt="img"></p>
<p>样本矩阵按行划分，将样本特征向量分布到不同的计算节点，由各计算节点完成自己所负责样本的点乘与求和计算，然后将计算结果进行归并，则实现了按行并行的 LR。按行并行的 LR 解决了样本数量的问题，但是实际情况中会存在针对高维特征向量进行逻辑回归的场景，仅仅按行进行并行处理，无法满足这类场景的需求，因此还需要按列将高维的特征向量拆分成若干小的向量进行求解。</p>
<p><img src="https://pic3.zhimg.com/v2-649613856d724587cfe627ef1870a826_r.jpg" alt="img"></p>
<p>并行计算总共会被分为两个并行化计算步骤和两个结果归并步骤：</p>
<p><strong>步骤一：</strong>各节点并行计算点乘，计算 <img src="https://www.zhihu.com/equation?tex=d_%7B(r,c),k,t%7D+=+W%5ET_%7Bc,t%7DX_%7B(r,c),k%7D+" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=+k=1,2%E2%80%A6M/m" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=d_%7B(r,c),k,t%7D" alt="[公式]"> 表示第 t 次迭代中节点 <img src="https://www.zhihu.com/equation?tex=(r,c)" alt="[公式]"> 上的第 k 个特征向量与特征权重分量的点乘， <img src="https://www.zhihu.com/equation?tex=W_%7Bc,t%7D" alt="[公式]"> 为第 t 次迭代中特征权重向量在第 c 列节点上的分量； <strong>步骤二：</strong>对行号相同的节点归并点乘结果：</p>
<p><img src="https://www.zhihu.com/equation?tex=d_%7Br,k,t%7D=W%5ET_t+X_%7Br,k%7D=%5Csum_%7Bc=1%7D%5En+d_%7B(r,c),k,t%7D=%5Csum_%7Bc=1%7D%5En+W_%7Bc,t%7D%5ETX_%7B(r,c),k%7D++%5C%5C" alt="[公式]"></p>
<p><img src="https://pic3.zhimg.com/v2-6b5f725a2893d4fa28a2ddb17db4c756_r.jpg" alt="img"></p>
<p><strong>步骤三：</strong>各节点独立算标量与特征向量相乘：</p>
<p><img src="https://www.zhihu.com/equation?tex=G_%7B(r,c),t%7D=%5Csum_%7Bk=1%7D%5E%7BM/m%7D%5B%5Csigma(y_%7Br,k%7Dd_%7Br,k,t%7D)-1%5Dy_%7Br,k%7DX_%7B(r,c),k%7D++%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=+G_%7B(r,c),t%7D" alt="[公式]"> 可以理解为由第 r 行节点上部分样本计算出的目标函数梯度向量在第 c 列节点上的分量。</p>
<p><strong>步骤四：</strong>对列号相同的节点进行归并：</p>
<p><img src="https://www.zhihu.com/equation?tex=G_%7Bc,t%7D=%5Csum_%7Br=1%7D%5Em+G_%7B(r,c),t%7D++%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=G_%7Bc,t%7D" alt="[公式]"> 就是目标函数的梯度向量 <img src="https://www.zhihu.com/equation?tex=G_t+" alt="[公式]"> 在第 c 列节点上的分量，对其进行归并得到目标函数的梯度向量：</p>
<p><img src="https://www.zhihu.com/equation?tex=G_t=%3CG_%7B1,t%7D,...,G_%7Bc,t%7D...G_%7Bn,t%7D%3E+%5C%5C" alt="[公式]"></p>
<p>这个过程如下图所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-a7cc0b79e62018e1a62dfcab5435e8f4_1440w.jpg" alt="img"></p>
<p>所以并行计算 LR 的流程如下所示。</p>
<p><img src="https://pic1.zhimg.com/v2-d93c4826068dbca6030cb7ca895102a0_r.jpg" alt="img"></p>
<p>所以并行 LR 实际上就是在求解损失函数最优解的过程中，针对寻找损失函数下降方向中的梯度方向计算作了并行化处理，而在利用梯度确定下降方向的过程中也可以采用并行化。</p>
<h2 id="2-与其他模型的对比"><a href="#2-与其他模型的对比" class="headerlink" title="2. 与其他模型的对比"></a>2. 与其他模型的对比</h2><h3 id="2-1-与线性回归"><a href="#2-1-与线性回归" class="headerlink" title="2.1 与线性回归"></a>2.1 与线性回归</h3><p>逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。</p>
<p><strong>我们需要明确 Sigmoid 函数到底起了什么作用：</strong></p>
<ul>
<li><strong>线性回归是在实数域范围内进行预测，而分类范围则需要在 [0,1]，逻辑回归减少了预测范围；</strong></li>
<li><strong>线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。</strong></li>
</ul>
<h3 id="2-2-与最大熵模型"><a href="#2-2-与最大熵模型" class="headerlink" title="2.2 与最大熵模型"></a>2.2 与最大熵模型</h3><p>逻辑回归和最大熵模型本质上没有区别，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。</p>
<p>首先进行符号定义：</p>
<ol>
<li><img src="https://www.zhihu.com/equation?tex=%5Cpi+(x)_u" alt="[公式]"> 表示，输入时 x，输出的 y=u 的概率；</li>
<li>A(u,v) 是一个指示函数，若 u=v，则 A(u,v)=1，否则 A(u,v)=0；</li>
<li>我们的目标就是从训练数据中，学习得到一个模型，使得 <img src="https://www.zhihu.com/equation?tex=+%5Cpi(x)_u" alt="[公式]"> 最大化，也就是输入 x，预测结果是 y 的概率最大，也就是使得 <img src="https://www.zhihu.com/equation?tex=+%5Cpi(x)_y" alt="[公式]"> 最大。</li>
</ol>
<p>对于逻辑回归而言：</p>
<p><img src="https://www.zhihu.com/equation?tex=P(Y=1%7Cx)+=+%5Cpi(x)_1+=%5Cdfrac%7Be%5E%7Bw+%5Ccdot+x%7D%7D%7B1+e%5E%7Bw+%5Ccdot+x%7D%7D+%5C%5C+++P(Y=0%7Cx)+=+%5Cpi(x)_0+=+1-%5Cpi(x)_1+%5C%5C" alt="[公式]"></p>
<p>我们这里可以用更泛化的形式来表示 π()：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cpi(x)_v=%5Cdfrac%7Be%5E%7Bw_v+%5Ccdot+x%7D%7D%7B%5Csum_%7Bu=1%7D%5Ek+e%5E%7Bw_u+%5Ccdot+x%7D%7D++%5C%5C" alt="[公式]"></p>
<p>回到我们的目标：令 <img src="https://www.zhihu.com/equation?tex=%5Cpi(x_i)y_i" alt="[公式]"> 最大，可以用极大似然估计的方法来求解。</p>
<p><img src="https://www.zhihu.com/equation?tex=L(w)=%5Cprod_%7Bi=1%7D%5En+%5Cpi(x_i)%7By_i%7D+%5C%5C+lnL(w)=%5Csum_%7Bi=1%7D%5En+ln(%5Cpi(x_i)%7By_i%7D)++%5C%5C" alt="[公式]"></p>
<p>然后我们求偏导：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+w_%7Bu,j%7D%7DlnL(w)=...=%5Csum_%7Bi=1,%5C;y_i=u%7D%5Enx_%7Bij%7D-%5Csum_%7Bi=1%7D%5Enx_%7Bij%7D%5Cpi(x_i)_u++%5C%5C" alt="[公式]"></p>
<p>另偏导数为 0：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi=1%7D%5Enx_%7Bij%7D%5Cpi(x_i)_u=%5Csum_%7Bi=1,%5C;y_i=u%7D%5Enx_%7Bij%7D,+(for%5C;all%5C;+u,j)++%5C%5C+" alt="[公式]"></p>
<p>使用 <img src="https://www.zhihu.com/equation?tex=A(u,y_i)+" alt="[公式]"> 这个函数，我们可以重写等式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi=1%7D%5Enx_%7Bij%7D%5Cpi(x_i)_u=%5Csum_%7Bi=1%7D%5En+A(u,y_i)x_%7Bij%7D,+(for%5C;all%5C;+u,j)++%5C%5C" alt="[公式]"></p>
<p>想要证明逻辑回归跟最大熵模型是等价的，那么，只要能够证明它们的 <img src="https://www.zhihu.com/equation?tex=+%5Cpi+()+" alt="[公式]"> 是相同，结论自然就出来了。现在，我们不知道最大熵模型的 <img src="https://www.zhihu.com/equation?tex=%5Cpi+()" alt="[公式]"> ，但是我们知道下面的一些性质：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cpi(x)_v%5Cgeq0+%5Cquad+always++%5C%5C+%5Csum_%7Bv=1%7D%5Ek%5Cpi(x)_v+=+1+%5Cquad+always+%5C%5C+%5Csum_%7Bi=1%7D%5Enx_%7Bij%7D%5Cpi(x_i)_u=%5Csum_%7Bi=1%7D%5En+A(u,y_i)x_%7Bij%7D,+%5Cquad(for%5C;all%5C;+u,j)+%5C%5C" alt="[公式]"></p>
<p>利用信息论，我们可以得到 <img src="https://www.zhihu.com/equation?tex=%5Cpi+()" alt="[公式]"> 的<strong>熵</strong>，定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Csum_%7Bv=1%7D%5Ek%5Csum_%7Bi=1%7D%5En%5Cpi(x_i)vlog%5B%5Cpi(x_i)_v%5D++%5C%5C" alt="[公式]"></p>
<p>现在，我们有了<strong>目标</strong>： <img src="https://www.zhihu.com/equation?tex=%5Csum+%5Cpi()" alt="[公式]"> 最大，也有了上面的4个<strong>约束条件</strong>。求解约束最优化问题，可以通过拉格朗日乘子，将约束最优化问题转换为<strong>无约束最优化</strong>的对偶问题。我们的拉格朗日式子可以写成如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=L=%5Csum_%7Bj=1%7D%5Em%5Csum_%7Bv=1%7D%5Ekw_%7Bv,j%7D(%5Csum_%7Bi=1%7D%5En%5Cpi(x_i)_vx_%7Bij%7D-A(v,y_i)x_%7Bij%7D)+%5C%5C++%5Csum_%7Bv=1%7D%5Ek%5Csum_%7Bi=1%7D%5En%5Cbeta_i(%5Cpi(x_i)_v-1)++%5C%5C+-%5Csum_%7Bv=1%7D%5Ek%5Csum_%7Bi=1%7D%5En+%5Cpi(x_i)_vlog%5B%5Cpi(x_i)_v%5D" alt="[公式]"></p>
<p>对 L 求偏导，得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cpi(x_i)_u%7DL=w_u+%5Ccdot+x_i+%5Cbeta_i-log%5B%5Cpi(x_i)_u%5D-1+%5C%5C" alt="[公式]"></p>
<p>令偏导 = 0，得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=w_u+%5Ccdot+x_i+%5Cbeta_i-log%5B%5Cpi(x_i)_u%5D-1=0+%5C%5C" alt="[公式]"></p>
<p>从而得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cpi(x_i)_u=e%5E%7Bw_u+%5Ccdot+x_i+%5Cbeta_i-1%7D++%5C%5C" alt="[公式]"></p>
<p>因为有约束条件：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bv=1%7D%5Ek+%5Cpi(x)_v+=+1++%5C%5C" alt="[公式]"></p>
<p>所以：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bv=1%7D%5Eke%5E%7Bw_v+%5Ccdot+x_i+%5Cbeta_i-1%7D=1+%5C%5C" alt="[公式]"></p>
<p>因此，可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=e%5E%5Cbeta=%5Cfrac%7B1%7D%7B%5Csum_%7Bv=1%7D%5Eke%5E%7Bw_v+%5Ccdot+x_i-1%7D%7D+%5C%5C" alt="[公式]"></p>
<p>把 <img src="https://www.zhihu.com/equation?tex=e%5E%5Cbeta" alt="[公式]"> 代入 <img src="https://www.zhihu.com/equation?tex=%5Cpi+()" alt="[公式]"> ，并且简化一下式子：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cpi(x)_u=%5Cfrac%7Be%5E%7Bw_u%5Ccdot+x%7D%7D%7B%5Csum_%7Bv=1%7D%5Ek+e%5E%7Bw_v+%5Ccdot+x%7D%7D+%5C%5C" alt="[公式]"></p>
<p>这就是逻辑回归中提到的那个泛化的式子，这就证明了逻辑回归是最大熵模型的一个特殊例子。到此，逻辑回归与最大熵模型的关系就解释完毕了。</p>
<h3 id="2-3-与-SVM"><a href="#2-3-与-SVM" class="headerlink" title="2.3 与 SVM"></a>2.3 与 SVM</h3><p>相同点：</p>
<ul>
<li>都是分类算法，本质上都是在找最佳分类超平面；</li>
<li>都是监督学习算法；</li>
<li>都是判别式模型，判别模型不关心数据是怎么生成的，它只关心数据之间的差别，然后用差别来简单对给定的一个数据进行分类；</li>
<li>都可以增加不同的正则项。</li>
</ul>
<p>不同点：</p>
<ul>
<li>LR 是一个统计的方法，SVM 是一个几何的方法；</li>
<li>SVM 的处理方法是只考虑 Support Vectors，也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重；</li>
<li>损失函数不同：LR 的损失函数是交叉熵，SVM 的损失函数是 HingeLoss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。对 HingeLoss 来说，其零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这是支持向量机最大的优势所在，对训练样本数目的依赖大减少，而且提高了训练效率；</li>
<li>LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布；</li>
<li>LR 可以产生概率，SVM 不能；</li>
<li>LR 不依赖样本之间的距离，SVM 是基于距离的；</li>
<li>LR 相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM 的理解和优化相对来说复杂一些，SVM 转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。</li>
</ul>
<h3 id="2-4-与朴素贝叶斯"><a href="#2-4-与朴素贝叶斯" class="headerlink" title="2.4 与朴素贝叶斯"></a>2.4 与朴素贝叶斯</h3><p>朴素贝叶斯和逻辑回归都属于分类模型，当朴素贝叶斯的条件概率 <img src="https://www.zhihu.com/equation?tex=P(X%7CY=c_k)" alt="[公式]"> 服从高斯分布时，它计算出来的 P(Y=1|X) 形式跟逻辑回归是一样的。</p>
<p>两个模型不同的地方在于：</p>
<ul>
<li>逻辑回归是判别式模型 p(y|x)，朴素贝叶斯是生成式模型 p(x,y)：判别式模型估计的是条件概率分布，给定观测变量 x 和目标变量 y 的条件模型，由数据直接学习决策函数 y=f(x) 或者条件概率分布 P(y|x) 作为预测的模型。判别方法关心的是对于给定的输入 x，应该预测什么样的输出 y；而生成式模型估计的是联合概率分布，基本思想是首先建立样本的联合概率概率密度模型 P(x,y)，然后再得到后验概率 P(y|x)，再利用它进行分类，生成式更关心的是对于给定输入 x 和输出 y 的生成关系；</li>
<li>朴素贝叶斯的前提是条件独立，每个特征权重独立，所以如果数据不符合这个情况，朴素贝叶斯的分类表现就没逻辑会好了。</li>
</ul>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title>随机抽样一致算法（RANdom SAmple Consensus,RANSAC）</title>
    <url>/2021/05/19/%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7%E4%B8%80%E8%87%B4%E7%AE%97%E6%B3%95%EF%BC%88RANdom-SAmple-Consensus-RANSAC%EF%BC%89/</url>
    <content><![CDATA[<p>一、RANSAC介绍<br>   随机抽样一致算法（RANdom SAmple Consensus,RANSAC）,采用迭代的方式从一组包含离群的被观测数据中估算出数学模型的参数。RANSAC算法假设数据中包含正确数据和异常数据（或称为噪声）。正确数据记为内点（inliers），异常数据记为外点（outliers）。同时RANSAC也假设，给定一组正确的数据，存在可以计算出符合这些数据的模型参数的方法。该算法核心思想就是随机性和假设性，随机性是根据正确数据出现概率去随机选取抽样数据，根据大数定律，随机性模拟可以近似得到正确结果。假设性是假设选取出的抽样数据都是正确数据，然后用这些正确数据通过问题满足的模型，去计算其他点，然后对这次结果进行一个评分。<br>二、算法基本思想<br>（1）要得到一个直线模型，需要两个点唯一确定一个直线方程。所以第一步随机选择两个点。<br>（2）通过这两个点，可以计算出这两个点所表示的模型方程y=ax+b。<br>（3）将所有的数据点套到这个模型中计算误差。<br>（4）找到所有满足误差阈值的点。<br>（5）然后我们再重复（1）~（4）这个过程，<strong>直到达到一定迭代次数后，选出那个被支持的最多的模型，作为问题的解</strong>。如下图所示</p>
<p><img src="https://img-blog.csdn.net/20161201151915050" alt="img"></p>
<p>可以发现，虽然这个数据集中外点和内点的比例几乎相等，但是RANSAC算法还是能找到最合适的解。这个问题如果使用最小二乘法进行优化，由于噪声数据的干扰，我们得到的结果肯定是一个错误的结果，如下图所示。<strong>这是由于最小二乘法是一个将外点参与讨论的代价优化问题，而RANSAC是一个使用内点进行优化的问题。</strong>经实验验证，对于包含80%误差的数据集，RANSAC的效果远优于直接的最小二乘法。</p>
<p><img src="https://img-blog.csdn.net/20161201151925746" alt="img"></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
</search>
